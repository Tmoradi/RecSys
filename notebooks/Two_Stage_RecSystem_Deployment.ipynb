{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "JlqmdU4BiM_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q protobuf==3.19.6\n",
        "!pip install -q tensorflow==2.11.1 --no-deps\n",
        "!pip install -q tensorflow-recommenders=='v0.7.3' --no-deps\n",
        "!pip install -q tensorflow-datasets==3.2.0 --no-deps\n",
        "!pip install -q tensorflow-metadata==0.22.2 --no-deps\n",
        "!pip install -q scann\n",
        "!pip install -q dill==0.3.6\n",
        "\n",
        "!pip install \"uvicorn[standard]>=0.12.0,<0.14.0\" fastapi~=0.63 -q\n",
        "!pip install \"google-cloud-storage>=1.26.0,<2.0.0dev\"\n",
        "!pip install google-cloud-aiplatform \"shapely<2\"\n",
        "!pip uninstall shapely-2.0.1 -y"
      ],
      "metadata": {
        "id": "3BzXWs7oVSrh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "411ed338-65f6-44bd-9848-33275c902952"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for httptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-cloud-storage<2.0.0dev,>=1.26.0\n",
            "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<2.0.0dev,>=1.26.0) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<2.0.0dev,>=1.26.0) (1.16.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<2.0.0dev,>=1.26.0) (2.17.3)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<2.0.0dev,>=1.26.0) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<2.0.0dev,>=1.26.0) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<2.0.0dev,>=1.26.0) (2.5.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<2.0.0dev,>=1.26.0) (3.19.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<2.0.0dev,>=1.26.0) (1.59.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<2.0.0dev,>=1.26.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<2.0.0dev,>=1.26.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<2.0.0dev,>=1.26.0) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2.0.0dev,>=1.26.0) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2.0.0dev,>=1.26.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2.0.0dev,>=1.26.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2.0.0dev,>=1.26.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2.0.0dev,>=1.26.0) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<2.0.0dev,>=1.26.0) (0.5.0)\n",
            "Installing collected packages: google-cloud-storage\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "Successfully installed google-cloud-storage-1.44.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.26.1-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.19.6)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.44.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.9.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.54.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
            "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Installing collected packages: shapely, grpc-google-iam-v1, google-cloud-resource-manager, google-cloud-aiplatform\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "Successfully installed google-cloud-aiplatform-1.26.1 google-cloud-resource-manager-1.10.1 grpc-google-iam-v1-0.12.6 shapely-1.8.5.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping shapely-2.0.1 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "metadata": {
        "id": "dHjp5kCeiQzV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "zDNJK1E7iyje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ecommerce dataset\n",
        "df_ecommerce = pd.read_csv('ecommerce.csv') # Contains transactions\n",
        "df_products = pd.read_csv('products.csv')   # Contains products"
      ],
      "metadata": {
        "id": "odG6GaIX2Wco"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataframe to tensors\n",
        "ds_ecommerce = tf.data.Dataset.from_tensor_slices(dict(df_ecommerce))\n",
        "ds_products = tf.data.Dataset.from_tensor_slices(dict(df_products))"
      ],
      "metadata": {
        "id": "HP4JrSmE2W2d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_ecommerce = tf.data.Dataset.from_tensor_slices(dict(df_ecommerce))\n",
        "# Select fields\n",
        "ds_ecommerce = ds_ecommerce.map(lambda x: {\n",
        "    'user_id': tf.strings.as_string(x['user_id']),\n",
        "    'product_id': tf.strings.as_string(x['product_id']),\n",
        "    'product_name': x['product_name'],\n",
        "    'product_description': x['product_description'],\n",
        "    'age': x['age'],\n",
        "    'search_query': x['search_query']\n",
        "})\n",
        "\n",
        "# ds_products = tf.data.Dataset.from_tensor_slices(dict(df_products))\n",
        "# ds_products = ds_products.batch(32).map(lambda x: tf.strings.as_string(x['product_id']))\n",
        "\n",
        "# Get all the user IDs\n",
        "user_ids = ds_ecommerce.batch(1000000).map(lambda x: x[\"user_id\"])\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids))).astype(str)\n",
        "\n",
        "# Get all the product IDs\n",
        "product_ids = ds_ecommerce.batch(1000000).map(lambda x: x[\"product_id\"])\n",
        "unique_product_ids = np.unique(np.concatenate(list(product_ids))).astype(str)"
      ],
      "metadata": {
        "id": "z-5OQB4P1M1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38d3d2f-7a35-4f37-fbc7-4f0cc7279590"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_products = tf.data.Dataset.from_tensor_slices(dict(df_products))\n",
        "ds_products = (\n",
        "    ds_products.map(lambda x: {\n",
        "        'product_id': tf.strings.as_string(x['product_id']),\n",
        "        'product_name': x['product_name'],\n",
        "        'product_description': x['product_description']\n",
        "    })\n",
        ")"
      ],
      "metadata": {
        "id": "ceaVjbtj8_hE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 05 - Ranker"
      ],
      "metadata": {
        "id": "tmdaJFVh6QIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserModelSearchData(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, unique_user_id_list, max_tokens=1000, output_sequence_length=30):\n",
        "    super().__init__()\n",
        "\n",
        "    # User ID Embedding\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=unique_user_id_list, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_id_list) + 1, 32),\n",
        "    ])\n",
        "\n",
        "    # User age\n",
        "    self.age_normalizer = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
        "\n",
        "    # Search Query Embedding\n",
        "    self.search_vectorization = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=max_tokens, output_sequence_length=output_sequence_length)\n",
        "    self.search_embedding = tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True)\n",
        "\n",
        "    # Fully-Connected Dense Network\n",
        "    self.dense = tf.keras.layers.Dense(32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    user_emb = self.user_embedding(inputs['user_id'])\n",
        "    age = self.age_normalizer(inputs['age'])\n",
        "\n",
        "    # Incorporate search history\n",
        "    search_seq = self.search_vectorization(inputs['search_query'])\n",
        "    search_emb = self.search_embedding(search_seq)\n",
        "    search_emb = tf.reduce_mean(search_emb, axis=1)\n",
        "\n",
        "    # Concatenate user features\n",
        "    concatenated = tf.concat([user_emb, tf.reshape(age, (-1, 1)), search_emb], axis=1)\n",
        "    return self.dense(concatenated)\n",
        "\n",
        "  def adapt(self, data):\n",
        "    age_data = data.map(lambda x: x['age'])\n",
        "    self.age_normalizer.adapt(age_data)\n",
        "    search_data = data.map(lambda x: x['search_query'])\n",
        "    self.search_vectorization.adapt(search_data)\n",
        "\n",
        "# User and Product models.\n",
        "class ProductModelMetaData(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, unique_product_ids, max_tokens=1000, output_sequence_length=30):\n",
        "    super().__init__()\n",
        "\n",
        "    # Product ID Embedding\n",
        "    self.product_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=unique_product_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_product_ids) + 1, 32)\n",
        "    ])\n",
        "\n",
        "    # Product Name Embedding\n",
        "    self.name_vectorization = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=max_tokens, output_sequence_length=output_sequence_length)\n",
        "    self.name_embedding = tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True)\n",
        "\n",
        "    # Product Description Embedding\n",
        "    self.description_vectorization = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=max_tokens, output_sequence_length=output_sequence_length)\n",
        "    self.description_embedding = tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True)\n",
        "\n",
        "    # Fully-Connected Dense Network\n",
        "    self.dense = tf.keras.layers.Dense(32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    product_emb = self.product_embedding(inputs['product_id'])\n",
        "\n",
        "    # Incorporate product description\n",
        "    name_seq = self.name_vectorization(inputs['product_name'])\n",
        "    name_emb = self.name_embedding(name_seq)\n",
        "    name_emb = tf.reduce_mean(name_emb, axis=1)\n",
        "\n",
        "    # Incorporate product description\n",
        "    description_seq = self.description_vectorization(inputs['product_description'])\n",
        "    description_emb = self.description_embedding(description_seq)\n",
        "    description_emb = tf.reduce_mean(description_emb, axis=1)\n",
        "\n",
        "    # Concatenate product features\n",
        "    concatenated = tf.concat([product_emb, name_emb, description_emb], axis=1)\n",
        "    return self.dense(concatenated)\n",
        "\n",
        "  def adapt(self, data):\n",
        "    description = data.map(lambda x: x['product_description'])\n",
        "    self.description_vectorization.adapt(description)\n",
        "    name = data.map(lambda x: x['product_name'])\n",
        "    self.name_vectorization.adapt(name)\n",
        "\n",
        "# Define the two-tower model.\n",
        "class TwoTowerModelRanker(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, user_model, product_model, task):\n",
        "    super().__init__()\n",
        "    self.user_model = user_model\n",
        "    self.product_model = product_model\n",
        "    self.task = task\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    user_embeddings = self.user_model(features)\n",
        "    positive_product_embeddings = self.product_model(features)\n",
        "    return self.task(user_embeddings, positive_product_embeddings)\n",
        "\n",
        "  def predict(self, features, training=False):\n",
        "    user_embeddings = self.user_model(features)\n",
        "    product_embeddings = self.product_model(features)\n",
        "    # Dot product\n",
        "    dot_products = tf.reduce_sum(user_embeddings * product_embeddings, axis=1)\n",
        "    # Apply sigmoid transformation\n",
        "    scores = tf.sigmoid(dot_products)\n",
        "    return scores\n",
        "\n",
        "# Instantiate and compile the model.\n",
        "user_model = UserModelSearchData(unique_user_ids)\n",
        "user_model.adapt(ds_ecommerce.batch(1000))\n",
        "\n",
        "product_model = ProductModelMetaData(unique_product_ids)\n",
        "product_model.adapt(ds_products.batch(1000))\n",
        "\n",
        "# Calculate embeddings for all products.\n",
        "product_embeddings = ds_products.batch(100).map(product_model)\n",
        "\n",
        "# Specify the task.\n",
        "task = tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(candidates=product_embeddings)\n",
        ")\n",
        "\n",
        "model = TwoTowerModelRanker(user_model, product_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "# Train for 3 epochs.\n",
        "model.fit(ds_ecommerce.batch(1000), epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_g_j70c6Trs",
        "outputId": "1bd9492a-d7f4-4532-d018-b4f09f930097"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 5s 204ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0538 - factorized_top_k/top_5_categorical_accuracy: 0.0684 - factorized_top_k/top_10_categorical_accuracy: 0.0772 - factorized_top_k/top_50_categorical_accuracy: 0.1116 - factorized_top_k/top_100_categorical_accuracy: 0.2256 - loss: 6946.8794 - regularization_loss: 0.0000e+00 - total_loss: 6946.8794\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1126ebcf10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show prediction\n",
        "for row in ds_ecommerce.batch(3).take(1):\n",
        "  print(model.predict(row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v1CFjSsFNr-",
        "outputId": "43f0c74d-5a3c-444f-a2da-737359ca9eed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.5238556  0.47241548 0.48426977], shape=(3,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 06 - Putting it Together"
      ],
      "metadata": {
        "id": "bMObEtxFP4V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN Index"
      ],
      "metadata": {
        "id": "WEybl8-1RZmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scann_retrieval = tfrs.layers.factorized_top_k.ScaNN(\n",
        "    model.user_model,\n",
        "    # Number of leaves (clusters)\n",
        "    num_leaves=100,\n",
        "    # Top 3 clusters to search from the query to the centroid\n",
        "    num_leaves_to_search=5,\n",
        ")\n",
        "\n",
        "# Add candidates in the index\n",
        "product_embeddings_to_index = ds_products.batch(1000).map(lambda x: model.product_model(x))\n",
        "scann_retrieval.index_from_dataset(product_embeddings_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKbZKpvvM4WC",
        "outputId": "9ae38fb2-6cac-40ac-8879-9ff50717e057"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7f11271ddb10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Data\n",
        "K = 10\n",
        "\n",
        "# Get some recommendations.\n",
        "user_id = tf.constant([\"1\"])  # user_id should be a string tensor\n",
        "user_age = tf.constant([25])  # user_age should be a numeric tensor\n",
        "user_search = tf.constant(['shirt'])\n",
        "user_query = {\"user_id\": user_id, \"age\": user_age, \"search_query\": user_search}\n",
        "\n",
        "# Generate candidates\n",
        "_, candidates = scann_retrieval(user_query, k=K)\n",
        "candidates = tf.strings.as_string(candidates)\n",
        "\n",
        "# Generate ranker scores\n",
        "filtered_candidates = ds_products.filter(lambda x: tf.math.reduce_any(tf.math.equal(x['product_id'], candidates)))"
      ],
      "metadata": {
        "id": "ikGl4-prICMU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(candidates)\n",
        "\n",
        "#tf.math.equal(ds_products['product_id'], candidates)\n",
        "print(tf.math.reduce_any([True, True, False, False]))\n",
        "print(list(filtered_candidates.as_numpy_iterator()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coo6buaoAW9o",
        "outputId": "ba57375e-fa9e-4c20-9616-e328c32a3365"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[b'199' b'225' b'259' b'354' b'189' b'133' b'184' b'370' b'146' b'11']], shape=(1, 10), dtype=string)\n",
            "tf.Tensor(True, shape=(), dtype=bool)\n",
            "[{'product_id': b'11', 'product_name': b'Practical Soft Mouse', 'product_description': b'The Nagasaki Lander is the trademarked name of several series of Nagasaki sport bikes, that started with the 1984 ABC800J'}, {'product_id': b'133', 'product_name': b'Ergonomic Wooden Sausages', 'product_description': b'The slim & simple Maple Gaming Keyboard from Dev Byte comes with a sleek body and 7- Color RGB LED Back-lighting for smart functionality'}, {'product_id': b'146', 'product_name': b'Refined Rubber Table', 'product_description': b'The Apollotech B340 is an affordable wireless mouse with reliable connectivity, 12 months battery life and modern design'}, {'product_id': b'184', 'product_name': b'Practical Wooden Keyboard', 'product_description': b'New ABC 13 9370, 13.3, 5th Gen CoreA5-8250U, 8GB RAM, 256GB SSD, power UHD Graphics, OS 10 Home, OS Office A & J 2016'}, {'product_id': b'189', 'product_name': b'Practical Fresh Fish', 'product_description': b'The Apollotech B340 is an affordable wireless mouse with reliable connectivity, 12 months battery life and modern design'}, {'product_id': b'199', 'product_name': b'Elegant Granite Chicken', 'product_description': b'Andy shoes are designed to keeping in mind durability as well as trends, the most stylish range of shoes & sandals'}, {'product_id': b'225', 'product_name': b'Generic Frozen Chicken', 'product_description': b'The Apollotech B340 is an affordable wireless mouse with reliable connectivity, 12 months battery life and modern design'}, {'product_id': b'259', 'product_name': b'Rustic Wooden Hat', 'product_description': b'Carbonite web goalkeeper gloves are ergonomically designed to give easy fit'}, {'product_id': b'354', 'product_name': b'Handcrafted Metal Car', 'product_description': b'The automobile layout consists of a front-engine design, with transaxle-type transmissions mounted at the rear of the engine and four wheel drive'}, {'product_id': b'370', 'product_name': b'Elegant Wooden Bike', 'product_description': b'The beautiful range of Apple Natural\\xc3\\xa9 that has an exciting mix of natural ingredients. With the Goodness of 100% Natural Ingredients'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert user_query to a tf.data.Dataset\n",
        "user_query_ds = tf.data.Dataset.from_tensor_slices(user_query)\n",
        "#print(list(user_query_ds.as_numpy_iterator()))\n",
        "\n",
        "# Repeat it the same number of times as the length of filtered_candidates\n",
        "user_query_ds = user_query_ds.repeat(K)\n",
        "\n",
        "# Zip the two datasets together\n",
        "combined_ds = tf.data.Dataset.zip((user_query_ds, filtered_candidates))\n",
        "print(combined_ds)\n",
        "\n",
        "# Now merge the data in each pair to make a single dictionary\n",
        "def merge_fn(user_data, product_data):\n",
        "    merged_data = user_data.copy()\n",
        "    merged_data.update(product_data)\n",
        "    return merged_data\n",
        "\n",
        "merged_ds = combined_ds.map(merge_fn)\n",
        "print(merged_ds)\n",
        "\n",
        "# Check the result\n",
        "result = []\n",
        "for item in merged_ds.batch(5):\n",
        "  _id = item['product_id'].numpy()\n",
        "  name = item['product_name'].numpy()\n",
        "  score = model.predict(item).numpy()\n",
        "  batch_result = [(i, n, s) for i, n, s in zip(_id, name, score)]\n",
        "  result += batch_result\n",
        "result = sorted(result, key=lambda x: x[2], reverse=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8hXehplOb0e",
        "outputId": "ad0121db-80cf-429e-ed1f-17839b89692d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<ZipDataset element_spec=({'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'age': TensorSpec(shape=(), dtype=tf.int32, name=None), 'search_query': TensorSpec(shape=(), dtype=tf.string, name=None)}, {'product_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_name': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_description': TensorSpec(shape=(), dtype=tf.string, name=None)})>\n",
            "<MapDataset element_spec={'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'age': TensorSpec(shape=(), dtype=tf.int32, name=None), 'search_query': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_name': TensorSpec(shape=(), dtype=tf.string, name=None), 'product_description': TensorSpec(shape=(), dtype=tf.string, name=None)}>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(b'259', b'Rustic Wooden Hat', 0.5912493),\n",
              " (b'184', b'Practical Wooden Keyboard', 0.54599226),\n",
              " (b'354', b'Handcrafted Metal Car', 0.53106886),\n",
              " (b'189', b'Practical Fresh Fish', 0.524371),\n",
              " (b'11', b'Practical Soft Mouse', 0.49213916),\n",
              " (b'370', b'Elegant Wooden Bike', 0.48564225),\n",
              " (b'146', b'Refined Rubber Table', 0.48314574),\n",
              " (b'133', b'Ergonomic Wooden Sausages', 0.4660032),\n",
              " (b'225', b'Generic Frozen Chicken', 0.4659229),\n",
              " (b'199', b'Elegant Granite Chicken', 0.4593957)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 07: GCP Deployment"
      ],
      "metadata": {
        "id": "tHDpSQ5_7Yrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**GCP set up**\n",
        "\n",
        "1.   Create a GCP account and activate the free tiers\n",
        "2.   Create a project named **mlemasterclass-deployment**\n",
        "1.   Create a new Cloud Storage bucket, titled **mlemasterclass-deployment-bucket** and specify the region **us-central1**\n",
        "1.   Make sure that the bucket is publicly accessed\n",
        "2.   Enable VertexAI APIs\n",
        "1.   Enable Compute Engine API\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zVzE3jKXxpq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declare the first GCP variables**"
      ],
      "metadata": {
        "id": "hl22myowzvqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"apt-honor-390612\"\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "MODEL_ARTIFACT_DIR = \"prediction-model\""
      ],
      "metadata": {
        "id": "liTXGFcnGiQ5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make sure the Cloud SDK uses the right project for all the commands in this notebook.**"
      ],
      "metadata": {
        "id": "RqJrW1ZgzrvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project $PROJECT_ID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDFAnNA6JLbU",
        "outputId": "a75631f1-6be1-4bfa-c455-886492b4df60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authenticate your Google Cloud account**\n",
        "\n",
        "This provides access to your Cloud Storage bucket and lets you submit training jobs and prediction requests."
      ],
      "metadata": {
        "id": "WfTOmxzx0CuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ],
      "metadata": {
        "id": "l7LbFjvTJeJw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declare the Bucket variables**"
      ],
      "metadata": {
        "id": "KEXJOyMQ0P0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = \"mlemasterclass-deployment-bucket\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ],
      "metadata": {
        "id": "WejAS54aKDBo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Show the stored folders/files**"
      ],
      "metadata": {
        "id": "tGJmLCJ50Vfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls -al $BUCKET_URI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whh6eO72KIio",
        "outputId": "c4edf54d-64a1-48bc-efd9-f68b3b314b00"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 gs://mlemasterclass-deployment-bucket/prediction-model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the ScaNN model to the Bucket\n",
        "\n",
        "Before you can deploy your model for serving, Vertex AI needs access to the following files in Cloud Storage:"
      ],
      "metadata": {
        "id": "MKgXoDNa0_fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir app"
      ],
      "metadata": {
        "id": "P6CdW-vr1JBE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(scann_retrieval, \"./app/index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IctFuYEX1DSE",
        "outputId": "f8d2c0e4-3e7a-41f1-ae3e-73bef02c9d7c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp -r ./app/index/* {BUCKET_URI}/{MODEL_ARTIFACT_DIR}/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnxv0x8d1McU",
        "outputId": "dcc98720-3bf5-4b53-9ee6-9bf1d07b23fc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://./app/index/fingerprint.pb [Content-Type=application/octet-stream]...\n",
            "Copying file://./app/index/saved_model.pb [Content-Type=application/octet-stream]...\n",
            "Copying file://./app/index/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "Copying file://./app/index/variables/variables.index [Content-Type=application/octet-stream]...\n",
            "| [4 files][721.3 KiB/721.3 KiB]                                                \n",
            "Operation completed over 4 objects/721.3 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls -al $BUCKET_URI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1XDj-HX1QNV",
        "outputId": "49416ee8-7596-496b-d105-6d8840d76ad1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 gs://mlemasterclass-deployment-bucket/prediction-model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loaded model"
      ],
      "metadata": {
        "id": "H9PDY8vI1p3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded  = tf.saved_model.load(f'{BUCKET_URI}/{MODEL_ARTIFACT_DIR}')\n",
        "trash, candidates = loaded(user_query)\n",
        "print(candidates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzcqmf1p1UGM",
        "outputId": "c32dc4d0-5aec-4563-bdf6-0b5bdefc9f3f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[199 225 259 354 189 133 184 370 146  11]], shape=(1, 10), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Vertex AI SDK for Python**"
      ],
      "metadata": {
        "id": "DE9YBY4J0lo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "GxRwd2P0K6KC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declare the required variables**"
      ],
      "metadata": {
        "id": "CMuWXd_h0pwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REPOSITORY = \"prediction-model-container\"\n",
        "IMAGE = \"model-api\"\n",
        "MODEL_DISPLAY_NAME = \"deployment-container-api\""
      ],
      "metadata": {
        "id": "NHC7x1irK9iZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the FastAPI code**\n",
        "\n",
        "To serve predictions from the classification model, build a FastAPI server application."
      ],
      "metadata": {
        "id": "DNDIjCd62S6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app/main.py\n",
        "from fastapi import FastAPI, Request\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "def json_to_tf(dictionnary):\n",
        "\n",
        "    transformed_dict = {}\n",
        "\n",
        "    for key, value in dictionnary.items():\n",
        "        transformed_dict[key] = tf.constant(value)\n",
        "\n",
        "    return(transformed_dict)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "scann_index = tf.saved_model.load(f'{os.environ[\"BUCKET_URI\"]}/{os.environ[\"MODEL_ARTIFACT_DIR\"]}')\n",
        "\n",
        "@app.get(\"/hello\")\n",
        "def greet_user():\n",
        "    return {\"message\": \"Hello! Welcome to MLE MasterClass!\"}\n",
        "\n",
        "@app.get(os.environ[\"AIP_HEALTH_ROUTE\"], status_code=200)\n",
        "def health():\n",
        "    return {\"message\": \"Model API is healthy!\"}\n",
        "\n",
        "@app.post(os.environ[\"AIP_PREDICT_ROUTE\"])\n",
        "async def predict(request: Request):\n",
        "    body = await request.json()\n",
        "    instances = body[\"instances\"]\n",
        "    trash, outputs = scann_index(json_to_tf(instances[0]))\n",
        "    outputs = [arr.tolist() for arr in outputs.numpy()]\n",
        "\n",
        "    return {\"predictions\": [class_num for class_num in outputs]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFHUVuu5adDD",
        "outputId": "1488b44f-7c14-4c2b-f23c-a62d7de2cf64"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add pre-start script**\n",
        "\n",
        "FastAPI executes the following script before starting up the server. The PORT environment variable is set to equal to AIP_HTTP_PORT in order to run FastAPI on the same port expected by Vertex AI."
      ],
      "metadata": {
        "id": "4wby27tp2dji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app/prestart.sh\n",
        "#!/bin/bash\n",
        "export PORT=$AIP_HTTP_PORT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzYAyqdohBdt",
        "outputId": "df8fe7e2-f481-4063-81d7-8112181bf428"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/prestart.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the test instances.json that store some data that will be used for prediction**"
      ],
      "metadata": {
        "id": "02kXvCTS2tZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instances.json\n",
        "{\n",
        "    \"instances\": [\n",
        "        {\"user_id\": [\"1\", \"2\"], \"age\": [25, 45], \"search_query\": [\"shirt\", \"long\"]}\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMVmdvCihYZW",
        "outputId": "304b0bee-f035-4902-a5d4-d3dad437e163"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instances.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the requirements.txt file that will be used by the Docker Image**"
      ],
      "metadata": {
        "id": "VRR5JLuf22wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "google-cloud-storage>=1.26.0,<2.0.0dev\n",
        "google-cloud-aiplatform\n",
        "shapely<2\n",
        "tensorflow==2.11.1\n",
        "protobuf==3.19.6\n",
        "tensorflow-recommenders==0.7.3\n",
        "tensorflow-datasets==3.2.0\n",
        "tensorflow-metadata==0.22.2\n",
        "scann\n",
        "dill==0.3.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE1QXIaSzJZe",
        "outputId": "efdf6168-09e6-47d6-b98f-1b25f267625f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the Dockerfile, using tiangolo/uvicorn-gunicorn-fastapi as a base image. This automatically runs FastAPI for you using Gunicorn and Uvicorn.\n",
        "\n",
        "Visit [https://fastapi.tiangolo.com/deployment/docker/](https://fastapi.tiangolo.com/deployment/docker/) to learn more."
      ],
      "metadata": {
        "id": "Apuwnrr-28GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile Dockerfile\n",
        "\n",
        "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
        "\n",
        "COPY ./app /app\n",
        "COPY requirements.txt requirements.txt\n",
        "\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "EXPOSE 8080\n",
        "\n",
        "ENV BUCKET_URI=gs://mlemasterclass-deployment-bucket\n",
        "ENV MODEL_ARTIFACT_DIR=prediction-model\n",
        "ENV AIP_HTTP_PORT=8080\n",
        "ENV AIP_HEALTH_ROUTE=health\n",
        "ENV AIP_PREDICT_ROUTE=predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g79xtdYShmQI",
        "outputId": "76174114-20aa-4ba4-dbc7-6c0cdd79130e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Push the container to artifact registry**\n",
        "\n",
        "Create your repository in the Artifact registry and push your container image to the repository. Run this below cell once to create the artifact repository."
      ],
      "metadata": {
        "id": "spvkIviO4NbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud artifacts repositories create {REPOSITORY} \\\n",
        "    --repository-format=docker \\\n",
        "    --location=$REGION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xv4K8U7hvvQ",
        "outputId": "4287586c-129c-468c-ba5b-a73138bae6c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push the image to the created artifact repository using Cloud-Build. (3~5 min)\n",
        "\n",
        "Note: The following command automatically considers the Dockerfile from the directory it is being run from."
      ],
      "metadata": {
        "id": "Pr5M6gIa4TQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud builds submit --region={REGION} --tag={REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piw4fJMQh3zN",
        "outputId": "ed57dd9e-c317-44c0-b8b9-554de8c08c79"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary tarball archive of 35 file(s) totalling 56.1 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://apt-honor-390612_cloudbuild/source/1687555310.795324-df5dca0f760c44bdb59e2c2504415592.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/apt-honor-390612/locations/us-central1/builds/b0bdd2e1-d3e0-4b07-af21-8cd595812b9d].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/b0bdd2e1-d3e0-4b07-af21-8cd595812b9d?project=637218062560 ].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"b0bdd2e1-d3e0-4b07-af21-8cd595812b9d\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://apt-honor-390612_cloudbuild/source/1687555310.795324-df5dca0f760c44bdb59e2c2504415592.tgz#1687555322834225\n",
            "Copying gs://apt-honor-390612_cloudbuild/source/1687555310.795324-df5dca0f760c44bdb59e2c2504415592.tgz#1687555322834225...\n",
            "/ [1 files][  7.0 MiB/  7.0 MiB]                                                \n",
            "Operation completed over 1 objects/7.0 MiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  58.91MB\n",
            "Step 1/10 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
            "python3.9: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
            "bd73737482dd: Already exists\n",
            "6710592d62aa: Already exists\n",
            "75256935197e: Already exists\n",
            "c1e5026c6457: Already exists\n",
            "f0016544b8b9: Already exists\n",
            "553887a17272: Pulling fs layer\n",
            "25a566c2ca56: Pulling fs layer\n",
            "29945ffb966e: Pulling fs layer\n",
            "f78f9d64b267: Pulling fs layer\n",
            "0234e09de7b3: Pulling fs layer\n",
            "d71a2448bdcc: Pulling fs layer\n",
            "767ee90adc97: Pulling fs layer\n",
            "dd6375a3847f: Pulling fs layer\n",
            "e4e8602bd7c5: Pulling fs layer\n",
            "0279cc972571: Pulling fs layer\n",
            "6ed714a519cb: Pulling fs layer\n",
            "af7b72583230: Pulling fs layer\n",
            "8067ca20e544: Pulling fs layer\n",
            "30bb41a4c6b2: Pulling fs layer\n",
            "f78f9d64b267: Waiting\n",
            "0234e09de7b3: Waiting\n",
            "d71a2448bdcc: Waiting\n",
            "767ee90adc97: Waiting\n",
            "dd6375a3847f: Waiting\n",
            "e4e8602bd7c5: Waiting\n",
            "0279cc972571: Waiting\n",
            "6ed714a519cb: Waiting\n",
            "af7b72583230: Waiting\n",
            "8067ca20e544: Waiting\n",
            "30bb41a4c6b2: Waiting\n",
            "29945ffb966e: Verifying Checksum\n",
            "29945ffb966e: Download complete\n",
            "25a566c2ca56: Verifying Checksum\n",
            "25a566c2ca56: Download complete\n",
            "f78f9d64b267: Verifying Checksum\n",
            "f78f9d64b267: Download complete\n",
            "553887a17272: Verifying Checksum\n",
            "553887a17272: Download complete\n",
            "0234e09de7b3: Verifying Checksum\n",
            "0234e09de7b3: Download complete\n",
            "d71a2448bdcc: Verifying Checksum\n",
            "d71a2448bdcc: Download complete\n",
            "dd6375a3847f: Verifying Checksum\n",
            "dd6375a3847f: Download complete\n",
            "e4e8602bd7c5: Verifying Checksum\n",
            "e4e8602bd7c5: Download complete\n",
            "767ee90adc97: Download complete\n",
            "6ed714a519cb: Verifying Checksum\n",
            "6ed714a519cb: Download complete\n",
            "0279cc972571: Download complete\n",
            "30bb41a4c6b2: Verifying Checksum\n",
            "30bb41a4c6b2: Download complete\n",
            "af7b72583230: Verifying Checksum\n",
            "af7b72583230: Download complete\n",
            "8067ca20e544: Verifying Checksum\n",
            "8067ca20e544: Download complete\n",
            "553887a17272: Pull complete\n",
            "25a566c2ca56: Pull complete\n",
            "29945ffb966e: Pull complete\n",
            "f78f9d64b267: Pull complete\n",
            "0234e09de7b3: Pull complete\n",
            "d71a2448bdcc: Pull complete\n",
            "767ee90adc97: Pull complete\n",
            "dd6375a3847f: Pull complete\n",
            "e4e8602bd7c5: Pull complete\n",
            "0279cc972571: Pull complete\n",
            "6ed714a519cb: Pull complete\n",
            "af7b72583230: Pull complete\n",
            "8067ca20e544: Pull complete\n",
            "30bb41a4c6b2: Pull complete\n",
            "Digest: sha256:af8efa0f8032c5f0e92933086edd199bc0d773c595daf6c5096f8f2fd8fd1b4f\n",
            "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
            " ---> 88c624e9f312\n",
            "Step 2/10 : COPY ./app /app\n",
            " ---> bdf42f4186c3\n",
            "Step 3/10 : COPY requirements.txt requirements.txt\n",
            " ---> 74fd03f54fd3\n",
            "Step 4/10 : RUN pip install -r requirements.txt\n",
            " ---> Running in 0fa72946f097\n",
            "Collecting google-cloud-storage<2.0.0dev,>=1.26.0\n",
            "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 4.4 MB/s eta 0:00:00\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.26.1-py2.py3-none-any.whl (2.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 38.3 MB/s eta 0:00:00\n",
            "Collecting shapely<2\n",
            "  Downloading Shapely-1.8.5.post1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 76.6 MB/s eta 0:00:00\n",
            "Collecting tensorflow==2.11.1\n",
            "  Downloading tensorflow-2.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 588.3/588.3 MB 1.9 MB/s eta 0:00:00\n",
            "Collecting protobuf==3.19.6\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 67.2 MB/s eta 0:00:00\n",
            "Collecting tensorflow-recommenders==0.7.3\n",
            "  Downloading tensorflow_recommenders-0.7.3-py3-none-any.whl (96 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.2/96.2 kB 16.6 MB/s eta 0:00:00\n",
            "Collecting tensorflow-datasets==3.2.0\n",
            "  Downloading tensorflow_datasets-3.2.0-py3-none-any.whl (3.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 88.9 MB/s eta 0:00:00\n",
            "Collecting tensorflow-metadata==0.22.2\n",
            "  Downloading tensorflow_metadata-0.22.2-py2.py3-none-any.whl (32 kB)\n",
            "Collecting scann\n",
            "  Downloading scann-1.2.9-cp39-cp39-manylinux_2_27_x86_64.whl (10.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/10.5 MB 51.1 MB/s eta 0:00:00\n",
            "Collecting dill==0.3.6\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 18.7 MB/s eta 0:00:00\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 439.2/439.2 kB 45.9 MB/s eta 0:00:00\n",
            "Collecting numpy>=1.20\n",
            "  Downloading numpy-1.25.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.7/17.7 MB 40.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.11.1->-r requirements.txt (line 4)) (4.6.3)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 13.8 MB/s eta 0:00:00\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 20.8 MB/s eta 0:00:00\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 66.4 MB/s eta 0:00:00\n",
            "Collecting packaging\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 8.2 MB/s eta 0:00:00\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 12.2 MB/s eta 0:00:00\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 91.1 MB/s eta 0:00:00\n",
            "Collecting six>=1.12.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/22.9 MB 32.6 MB/s eta 0:00:00\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 10.1 MB/s eta 0:00:00\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 54.7 MB/s eta 0:00:00\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.56.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 57.9 MB/s eta 0:00:00\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from tensorflow==2.11.1->-r requirements.txt (line 4)) (58.1.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 83.8 MB/s eta 0:00:00\n",
            "Collecting requests>=2.19.0\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 9.5 MB/s eta 0:00:00\n",
            "Collecting future\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 52.6 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 13.5 MB/s eta 0:00:00\n",
            "Collecting promise\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting attrs>=18.1.0\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 9.8 MB/s eta 0:00:00\n",
            "Collecting googleapis-common-protos\n",
            "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 30.4 MB/s eta 0:00:00\n",
            "Collecting google-api-core<3.0dev,>=1.29.0\n",
            "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.5/120.5 kB 19.9 MB/s eta 0:00:00\n",
            "Collecting google-auth<3.0dev,>=1.25.0\n",
            "  Downloading google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.5/181.5 kB 26.9 MB/s eta 0:00:00\n",
            "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 kB 10.3 MB/s eta 0:00:00\n",
            "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
            "  Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.1/48.1 kB 7.4 MB/s eta 0:00:00\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl (321 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.3/321.3 kB 39.8 MB/s eta 0:00:00\n",
            "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
            "  Downloading google_cloud_bigquery-3.11.2-py2.py3-none-any.whl (219 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.4/219.4 kB 30.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.11.1->-r requirements.txt (line 4)) (0.40.0)\n",
            "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
            "  Downloading grpcio_status-1.56.0-py3-none-any.whl (5.1 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting urllib3<2.0\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.1/143.1 kB 23.6 MB/s eta 0:00:00\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 24.8 MB/s eta 0:00:00\n",
            "Collecting python-dateutil<3.0dev,>=2.7.2\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 34.7 MB/s eta 0:00:00\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
            "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==3.2.0->-r requirements.txt (line 7)) (2023.5.7)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.2/199.2 kB 29.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==3.2.0->-r requirements.txt (line 7)) (3.4)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 16.7 MB/s eta 0:00:00\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 242.5/242.5 kB 33.3 MB/s eta 0:00:00\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 58.0 MB/s eta 0:00:00\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 65.2 MB/s eta 0:00:00\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
            "  Downloading grpcio_status-1.54.2-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.54.0-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.53.1-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.53.0-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.9/83.9 kB 15.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.1->-r requirements.txt (line 4)) (2.1.3)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 26.5 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: future, promise\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492035 sha256=72797dad3d79694309992cdf5a02a3962ea11c03885ec81d2a4ed4c25773deea\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/5d/6a/2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
            "  Building wheel for promise (setup.py): started\n",
            "  Building wheel for promise (setup.py): finished with status 'done'\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21501 sha256=894befdf6d0cdac3aba87e2e7cc78a3813bd2484e765c5a7316848403594c681\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
            "Successfully built future promise\n",
            "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, zipp, wrapt, werkzeug, urllib3, tqdm, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, shapely, pyasn1, protobuf, packaging, oauthlib, numpy, keras, grpcio, google-crc32c, gast, future, dill, charset-normalizer, cachetools, attrs, absl-py, rsa, requests, python-dateutil, pyasn1-modules, proto-plus, promise, opt-einsum, importlib-metadata, h5py, googleapis-common-protos, google-resumable-media, google-pasta, astunparse, tensorflow-metadata, requests-oauthlib, markdown, grpcio-status, google-auth, tensorflow-datasets, grpc-google-iam-v1, google-auth-oauthlib, google-api-core, tensorboard, google-cloud-core, tensorflow, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, tensorflow-recommenders, scann, google-cloud-aiplatform\n",
            "Successfully installed absl-py-1.4.0 astunparse-1.6.3 attrs-23.1.0 cachetools-5.3.1 charset-normalizer-3.1.0 dill-0.3.6 flatbuffers-23.5.26 future-0.18.3 gast-0.4.0 google-api-core-2.11.1 google-auth-2.20.0 google-auth-oauthlib-0.4.6 google-cloud-aiplatform-1.26.1 google-cloud-bigquery-3.11.2 google-cloud-core-2.3.2 google-cloud-resource-manager-1.10.1 google-cloud-storage-1.44.0 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.1 grpc-google-iam-v1-0.12.6 grpcio-1.56.0 grpcio-status-1.48.2 h5py-3.9.0 importlib-metadata-6.7.0 keras-2.11.0 libclang-16.0.0 markdown-3.4.3 numpy-1.25.0 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 promise-2.3 proto-plus-1.22.3 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 python-dateutil-2.8.2 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 scann-1.2.9 shapely-1.8.5.post1 six-1.16.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.1 tensorflow-datasets-3.2.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.32.0 tensorflow-metadata-0.22.2 tensorflow-recommenders-0.7.3 termcolor-2.3.0 tqdm-4.65.0 urllib3-1.26.16 werkzeug-2.3.6 wrapt-1.15.0 zipp-3.15.0\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91m\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
            "[notice] To update, run: pip install --upgrade pip\n",
            "\u001b[0mRemoving intermediate container 0fa72946f097\n",
            " ---> f7e4cb0ddeaf\n",
            "Step 5/10 : EXPOSE 8080\n",
            " ---> Running in 52566bb9c75b\n",
            "Removing intermediate container 52566bb9c75b\n",
            " ---> a6146d9e1ea2\n",
            "Step 6/10 : ENV BUCKET_URI=gs://mlemasterclass-deployment-bucket\n",
            " ---> Running in 10ce454cf4dc\n",
            "Removing intermediate container 10ce454cf4dc\n",
            " ---> 14edba632578\n",
            "Step 7/10 : ENV MODEL_ARTIFACT_DIR=prediction-model\n",
            " ---> Running in 6b282de3b898\n",
            "Removing intermediate container 6b282de3b898\n",
            " ---> 77765dfe5b86\n",
            "Step 8/10 : ENV AIP_HTTP_PORT=8080\n",
            " ---> Running in 2b5ec1873553\n",
            "Removing intermediate container 2b5ec1873553\n",
            " ---> 4f453f4af6d6\n",
            "Step 9/10 : ENV AIP_HEALTH_ROUTE=health\n",
            " ---> Running in 242ae34f3a92\n",
            "Removing intermediate container 242ae34f3a92\n",
            " ---> c7e070423025\n",
            "Step 10/10 : ENV AIP_PREDICT_ROUTE=predict\n",
            " ---> Running in 0475556f39d5\n",
            "Removing intermediate container 0475556f39d5\n",
            " ---> 27b76774a3ac\n",
            "Successfully built 27b76774a3ac\n",
            "Successfully tagged us-central1-docker.pkg.dev/apt-honor-390612/prediction-model-container/model-api:latest\n",
            "PUSH\n",
            "Pushing us-central1-docker.pkg.dev/apt-honor-390612/prediction-model-container/model-api\n",
            "The push refers to repository [us-central1-docker.pkg.dev/apt-honor-390612/prediction-model-container/model-api]\n",
            "442aa15ef31d: Preparing\n",
            "4713bfd157c6: Preparing\n",
            "f5bdaaf5802e: Preparing\n",
            "522bc1f4504d: Preparing\n",
            "24e633750075: Preparing\n",
            "1be52a193eb8: Preparing\n",
            "319afb297dfd: Preparing\n",
            "b8b43487bfc0: Preparing\n",
            "18045421428a: Preparing\n",
            "04fd58c30ae2: Preparing\n",
            "a4c230ef5c8d: Preparing\n",
            "0295a7fc415e: Preparing\n",
            "e7eb028ba956: Preparing\n",
            "aa4f0adaf89d: Preparing\n",
            "f9db5ba632e2: Preparing\n",
            "9123ac0d1254: Preparing\n",
            "1031abfec3f3: Preparing\n",
            "ea9a66bcf3b5: Preparing\n",
            "d140420135e3: Preparing\n",
            "b4b4f5c5ff9f: Preparing\n",
            "b0df24a95c80: Preparing\n",
            "974e52a24adf: Preparing\n",
            "aa4f0adaf89d: Waiting\n",
            "f9db5ba632e2: Waiting\n",
            "9123ac0d1254: Waiting\n",
            "1031abfec3f3: Waiting\n",
            "ea9a66bcf3b5: Waiting\n",
            "d140420135e3: Waiting\n",
            "b4b4f5c5ff9f: Waiting\n",
            "b0df24a95c80: Waiting\n",
            "974e52a24adf: Waiting\n",
            "1be52a193eb8: Waiting\n",
            "319afb297dfd: Waiting\n",
            "b8b43487bfc0: Waiting\n",
            "18045421428a: Waiting\n",
            "04fd58c30ae2: Waiting\n",
            "a4c230ef5c8d: Waiting\n",
            "0295a7fc415e: Waiting\n",
            "e7eb028ba956: Waiting\n",
            "24e633750075: Layer already exists\n",
            "522bc1f4504d: Layer already exists\n",
            "319afb297dfd: Layer already exists\n",
            "1be52a193eb8: Layer already exists\n",
            "b8b43487bfc0: Layer already exists\n",
            "18045421428a: Layer already exists\n",
            "04fd58c30ae2: Layer already exists\n",
            "a4c230ef5c8d: Layer already exists\n",
            "0295a7fc415e: Layer already exists\n",
            "f5bdaaf5802e: Pushed\n",
            "e7eb028ba956: Layer already exists\n",
            "4713bfd157c6: Pushed\n",
            "f9db5ba632e2: Layer already exists\n",
            "9123ac0d1254: Layer already exists\n",
            "1031abfec3f3: Layer already exists\n",
            "aa4f0adaf89d: Layer already exists\n",
            "ea9a66bcf3b5: Layer already exists\n",
            "b4b4f5c5ff9f: Layer already exists\n",
            "d140420135e3: Layer already exists\n",
            "b0df24a95c80: Layer already exists\n",
            "974e52a24adf: Layer already exists\n",
            "442aa15ef31d: Pushed\n",
            "latest: digest: sha256:ed7b4601abb1d6b4b668293de96582371f0ce11e10c2b49c0d31db4abb0ff0bd size: 4925\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                          IMAGES                                                                                      STATUS\n",
            "b0bdd2e1-d3e0-4b07-af21-8cd595812b9d  2023-06-23T21:22:06+00:00  4M37S     gs://apt-honor-390612_cloudbuild/source/1687555310.795324-df5dca0f760c44bdb59e2c2504415592.tgz  us-central1-docker.pkg.dev/apt-honor-390612/prediction-model-container/model-api (+1 more)  SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deploy to Vertex AI**\n",
        "\n",
        "**Create Vertex AI model using artifact uri**\n",
        "\n",
        "Use the Python SDK to upload and deploy your model from the artifact registry. (2~3 min)"
      ],
      "metadata": {
        "id": "m7_Fn7034XpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.cloud import aiplatform\n",
        "#aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "model = aiplatform.Model.upload(display_name=MODEL_DISPLAY_NAME,\n",
        "                                artifact_uri=f\"{BUCKET_URI}/{MODEL_ARTIFACT_DIR}\",\n",
        "                                serving_container_image_uri=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWAJsn4rP03Z",
        "outputId": "97c98da5-e527-4ad4-8178-7d5f208038df"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create Model backing LRO: projects/637218062560/locations/us-central1/models/5153131723432656896/operations/4463811806254596096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/637218062560/locations/us-central1/models/5153131723432656896/operations/4463811806254596096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created. Resource name: projects/637218062560/locations/us-central1/models/5153131723432656896@1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/637218062560/locations/us-central1/models/5153131723432656896@1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To use this Model in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model = aiplatform.Model('projects/637218062560/locations/us-central1/models/5153131723432656896@1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/637218062560/locations/us-central1/models/5153131723432656896@1')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploy the model to Vertex AI Endpoints**\n",
        "\n",
        "Deploy the model to a Vertex AI Endpoint. After this step completes, the model is deployed and ready for online predictions. (~5-10 min)"
      ],
      "metadata": {
        "id": "0yEdFqUm4hiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCui2J7CrUO3",
        "outputId": "c5d19db8-b7f1-4140-dbdf-884ef1ae30c7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Endpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Endpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create Endpoint backing LRO: projects/637218062560/locations/us-central1/endpoints/3878122646700818432/operations/8033477460899135488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/637218062560/locations/us-central1/endpoints/3878122646700818432/operations/8033477460899135488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoint created. Resource name: projects/637218062560/locations/us-central1/endpoints/3878122646700818432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/637218062560/locations/us-central1/endpoints/3878122646700818432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To use this Endpoint in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "endpoint = aiplatform.Endpoint('projects/637218062560/locations/us-central1/endpoints/3878122646700818432')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/637218062560/locations/us-central1/endpoints/3878122646700818432')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploying model to Endpoint : projects/637218062560/locations/us-central1/endpoints/3878122646700818432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/637218062560/locations/us-central1/endpoints/3878122646700818432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploy Endpoint model backing LRO: projects/637218062560/locations/us-central1/endpoints/3878122646700818432/operations/424082940503261184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/637218062560/locations/us-central1/endpoints/3878122646700818432/operations/424082940503261184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoint model deployed. Resource name: projects/637218062560/locations/us-central1/endpoints/3878122646700818432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/637218062560/locations/us-central1/endpoints/3878122646700818432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Request predictions**\n",
        "\n",
        "Send online requests to the model deployed to the endpoint and get predictions.\n",
        "\n",
        "**Using Python SDK**\n",
        "\n",
        "Get predictions from the endpoint for a sample input using python SDK."
      ],
      "metadata": {
        "id": "X7-O3xka4pL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON from a file\n",
        "with open('instances.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Access the loaded data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Psl7tTtqEt",
        "outputId": "dfb8bfdf-f525-4f13-e343-a5870e3eba62"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instances': [{'user_id': ['1', '2'], 'age': [25, 45], 'search_query': ['shirt', 'long']}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint.predict(instances= data['instances'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9OZAJ1utxDh",
        "outputId": "34b88f37-acad-4e43-bcf0-921b7a3696a3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(predictions=[[199.0, 225.0, 259.0, 354.0, 189.0, 133.0, 184.0, 370.0, 146.0, 11.0], [225.0, 214.0, 242.0, 252.0, 11.0, 146.0, 370.0, 264.0, 401.0, 258.0]], deployed_model_id='8073355441968513024', model_version_id='1', model_resource_name='projects/637218062560/locations/us-central1/models/5153131723432656896', explanations=None)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using REST**\n",
        "\n",
        "Get predictions from the endpoint using curl request."
      ],
      "metadata": {
        "id": "dafLzJIt43yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENDPOINT_ID = endpoint.name\n",
        "print(\"ENDPOINT_ID :\", ENDPOINT_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ7DePaG44wm",
        "outputId": "5f887ff9-9803-4241-8fc4-e1e4c3f945e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENDPOINT_ID : 3878122646700818432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -X POST \\\n",
        "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d @instances.json \\\n",
        "https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBANCdxZGG86",
        "outputId": "35f628e1-b8cb-4f41-daf1-2998320fe3f2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"predictions\": [\n",
            "    [\n",
            "      199,\n",
            "      225,\n",
            "      259,\n",
            "      354,\n",
            "      189,\n",
            "      133,\n",
            "      184,\n",
            "      370,\n",
            "      146,\n",
            "      11\n",
            "    ],\n",
            "    [\n",
            "      225,\n",
            "      214,\n",
            "      242,\n",
            "      252,\n",
            "      11,\n",
            "      146,\n",
            "      370,\n",
            "      264,\n",
            "      401,\n",
            "      258\n",
            "    ]\n",
            "  ],\n",
            "  \"deployedModelId\": \"8073355441968513024\",\n",
            "  \"model\": \"projects/637218062560/locations/us-central1/models/5153131723432656896\",\n",
            "  \"modelDisplayName\": \"deployment-container-api\",\n",
            "  \"modelVersionId\": \"1\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning up**\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) project you used for the tutorial."
      ],
      "metadata": {
        "id": "amsLB4Qi5BEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice**\n",
        "\n",
        "1.   Try to deploy the Docker Container on localhost using docker build, docker run commands (dont forget to set the env variables required by the FastAPI (AIP_PREDICT_ROUTE, etc...)\n",
        "2.   Try to add the ranker module to the application and deploy the new application locally and on GCP\n",
        "\n"
      ],
      "metadata": {
        "id": "1-ICr5giI3HB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bIHa_jycJca_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}